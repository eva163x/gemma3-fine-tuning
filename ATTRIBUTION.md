# Attribution

## Dataset
- **MATH Competition Dataset**: qwedsacf/competition_math
  - Source: Kaggle/HuggingFace Datasets
  - License: MIT
  - Used for training, validation, and testing

## Pre-trained Model
- **Gemma-3-270m-it**: google/gemma-3-270m-it
  - Source: Google/HuggingFace Model Hub
  - License: Gemma Terms of Use
  - Used as base model for fine-tuning

## Libraries and Frameworks
- **PyTorch**: Deep learning framework
- **Transformers**: HuggingFace model library
- **PEFT**: Parameter-Efficient Fine-Tuning library for LoRA
- **Datasets**: HuggingFace dataset library

## AI Assistance
- **Claude (Anthropic)**: data cleaning pipeline starter code provider, LossTrackerCallback class starter code provider

## Code References
- SFT pipeline inspired from CS372 HW6 
- LoRA implementation inspired by HuggingFace PEFT documentation
- Data preprocessing adapted from MATH dataset examples
- Evaluation metrics based on standard NLP practices
- Plotting curves base code from CS372 HW6

## External Resources
- LaTeX cleaning regex patterns: community best practices
